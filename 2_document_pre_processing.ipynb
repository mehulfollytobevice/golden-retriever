{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2a6713-b1ab-49ed-a46d-b65fcbff7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c224135-daf4-4793-b0b9-4000cf031720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html_files(base_path, include_folders):\n",
    "    all_files = []\n",
    "    for folder in include_folders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            # Exclude directories starting with an underscore\n",
    "            dirs[:] = [d for d in dirs if not d.startswith('_')]\n",
    "            for file in files:\n",
    "                if (file.endswith('.html')) and file != 'index.html':\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b854d6-7bf9-49ed-8329-77e0dbb80d26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4a5c34-fdbb-4e5c-b9b2-66582cc8b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting relevant file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481c1424-afa8-4ea5-bd03-25f768183cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_file_path = \"../documents/pandas_docs/\"\n",
    "pandas_include_folders = ['development', 'getting_started', 'reference/api', 'user_guide', 'whatsnew']\n",
    "pandas_html_files = load_html_files(pandas_file_path, pandas_include_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be0f863-f6aa-40a8-9f70-058f9e4daf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pandas_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b4d735-8ddb-4b45-a9a4-de882ba6868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for i in range(len(pandas_html_files)):\n",
    "#     if 'user_guide/' in pandas_html_files[i]:\n",
    "#         c+=1\n",
    "#         print(pandas_html_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01263623-0d5f-4aba-814c-773a36fa8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting content from html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b44ac76a-abbb-4444-8ef7-d96df83976e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_html_file(html_file):\n",
    "#   loader = UnstructuredHTMLLoader(html_file)\n",
    "#   return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2bb2746-085a-42ea-b099-eef98bb2dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_file(html_file):\n",
    "    loader = BSHTMLLoader(html_file, open_encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    \n",
    "    for doc in data:\n",
    "        if 'soup' in doc.metadata:\n",
    "            soup = doc.metadata['soup']\n",
    "        else:\n",
    "            with open(html_file, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "        \n",
    "        target_main = soup.find('div', class_='bd-article-container')\n",
    "        if target_main:\n",
    "            # Remove 'admonition seealso' div, 'prev-next-footer' footer, and 'header-article-items header-article__inner' div\n",
    "            for element in target_main.find_all(['div', 'footer'], class_=['admonition seealso', 'prev-next-footer', 'header-article-items header-article__inner']):\n",
    "                element.decompose()\n",
    "\n",
    "            # Extract and format the text content\n",
    "            formatted_text = []\n",
    "            seen_content = set()  # To keep track of unique content\n",
    "            for element in target_main.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'pre', 'code', 'dt']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if text and text not in seen_content:\n",
    "                    if element.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'dt']:\n",
    "                        formatted_text.append(f'\\n{text}\\n')\n",
    "                    elif element.name == 'pre':\n",
    "                        formatted_text.append(f'\\n{element.get_text()}\\n')\n",
    "                    elif element.name == 'code':\n",
    "                        formatted_text.append(text)\n",
    "                    seen_content.add(text)\n",
    "            \n",
    "            doc.page_content = ''.join(formatted_text).strip()\n",
    "        else:\n",
    "            doc.page_content = \"No content found in the specified main tag.\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parallel_load(html_files):\n",
    "  with multiprocessing.Pool() as pool:\n",
    "    results = list(tqdm(pool.imap(process_html_file, html_files), total=len(html_files), desc=\"Loading files\"))\n",
    "    return [item for sublist in results for item in sublist]  # Flatten the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b31d8326-1b36-4d7c-ae77-e8e18798634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 2366/2366 [00:07<00:00, 329.15it/s]\n"
     ]
    }
   ],
   "source": [
    "pandas_data = parallel_load(pandas_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4882659b-a595-42d4-a075-32bff9954e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pandas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d73d57e-44fe-4618-8787-f3de2ce1a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying URL source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf9e5b0b-4762-40cd-9ba6-893d3898e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the old and new base URLs\n",
    "pandas_old_base_url = \"../documents/pandas_docs\"\n",
    "pandas_new_base_url = \"https://pandas.pydata.org/docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "35c1c70b-965c-4ab7-83b2-7c9f946b5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each document and update the source in the metadata\n",
    "for doc in pandas_data:\n",
    "    if 'source' in doc.metadata:\n",
    "        doc.metadata['source'] = doc.metadata['source'].replace(pandas_old_base_url, pandas_new_base_url)\n",
    "\n",
    "# Now, pandas_docs contains updated sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a2b5c2b-ea59-434d-941d-b513ee5c8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='pandas.DataFrame.to_timestamp#\n",
      "\n",
      "DataFrame.to_timestamp(freq=None,how='start',axis=0,copy=None)[source]#\n",
      "\n",
      "Cast to DatetimeIndex of timestamps, atbeginningof period.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "freqstr, default frequency of PeriodIndex\n",
      "\n",
      "Desired frequency.\n",
      "\n",
      "how{‘s’, ‘e’, ‘start’, ‘end’}\n",
      "\n",
      "Convention for converting period to timestamp; start of period\n",
      "vs. end.\n",
      "\n",
      "axis{0 or ‘index’, 1 or ‘columns’}, default 0\n",
      "\n",
      "The axis to convert (the index by default).\n",
      "\n",
      "copybool, default True\n",
      "\n",
      "If False then underlying input data is not copied.\n",
      "\n",
      "Note\n",
      "\n",
      "Thecopykeyword will change behavior in pandas 3.0.Copy-on-Writewill be enabled by default, which means that all methods with acopykeyword will use a lazy copy mechanism to defer the copy and\n",
      "ignore thecopykeyword. Thecopykeyword will be removed in a\n",
      "future version of pandas.\n",
      "\n",
      "You can already get the future behavior and improvements through\n",
      "enabling copy on writepd.options.mode.copy_on_write=True\n",
      "pd.options.mode.copy_on_write=True\n",
      "Returns:\n",
      "\n",
      "DataFrame\n",
      "\n",
      "The DataFrame has a DatetimeIndex.\n",
      "\n",
      "Examples\n",
      "\n",
      ">>> idx = pd.PeriodIndex(['2023', '2024'], freq='Y')\n",
      ">>> d = {'col1': [1, 2], 'col2': [3, 4]}\n",
      ">>> df1 = pd.DataFrame(data=d, index=idx)\n",
      ">>> df1\n",
      "      col1   col2\n",
      "2023     1      3\n",
      "2024     2      4\n",
      "\n",
      "\n",
      "The resulting timestamps will be at the beginning of the year in this case\n",
      "\n",
      ">>> df1 = df1.to_timestamp()\n",
      ">>> df1\n",
      "            col1   col2\n",
      "2023-01-01     1      3\n",
      "2024-01-01     2      4\n",
      ">>> df1.index\n",
      "DatetimeIndex(['2023-01-01', '2024-01-01'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "\n",
      "Usingfreqwhich is the offset that the Timestamps will have\n",
      "\n",
      ">>> df2 = pd.DataFrame(data=d, index=idx)\n",
      ">>> df2 = df2.to_timestamp(freq='M')\n",
      ">>> df2\n",
      "            col1   col2\n",
      "2023-01-31     1      3\n",
      "2024-01-31     2      4\n",
      ">>> df2.index\n",
      "DatetimeIndex(['2023-01-31', '2024-01-31'], dtype='datetime64[ns]', freq=None)' metadata={'source': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_timestamp.html', 'title': 'pandas.DataFrame.to_timestamp — pandas 2.2.2 documentation'}\n"
     ]
    }
   ],
   "source": [
    "print(pandas_data[265])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d425c28e-c97b-40fa-904d-a85f1737777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='pandas.tseries.offsets.CustomBusinessDay.is_month_start#\n",
      "\n",
      "CustomBusinessDay.is_month_start(ts)#\n",
      "\n",
      "Return boolean whether a timestamp occurs on the month start.\n",
      "\n",
      "Examples\n",
      "\n",
      ">>> ts = pd.Timestamp(2022, 1, 1)\n",
      ">>> freq = pd.offsets.Hour(5)\n",
      ">>> freq.is_month_start(ts)\n",
      "True' metadata={'source': 'https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.CustomBusinessDay.is_month_start.html', 'title': 'pandas.tseries.offsets.CustomBusinessDay.is_month_start — pandas 2.2.2 documentation'}\n"
     ]
    }
   ],
   "source": [
    "print(pandas_data[1659])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26a2e2c2-e68a-48ab-afc0-0e5bd63594e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 105, 126, 172, 244, 414, 435, 436, 634, 754, 884, 906, 908, 1149, 1162, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1321, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1467, 1468, 1510, 2264]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_no_content = []\n",
    "for i in range(len(pandas_data)):\n",
    "    if pandas_data[i].page_content == 'No content found in the specified main tag.':\n",
    "        docs_with_no_content.append(i)\n",
    "        \n",
    "print(docs_with_no_content)\n",
    "len(docs_with_no_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3153afaf-054d-421b-bb24-1363fd95313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='No content found in the specified main tag.' metadata={'source': 'https://pandas.pydata.org/docs/reference/api/pandas.core.window.Rolling.std.html', 'title': ''}\n"
     ]
    }
   ],
   "source": [
    "print(pandas_data[1361])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b839d8be-c9fc-469d-8683-776287c4010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the documents with no content\n",
    "# If pandas_data is a list:\n",
    "pandas_data = [doc for i, doc in enumerate(pandas_data) if i not in docs_with_no_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "45a3c3c4-70da-4139-874f-dd304d639fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2286"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pandas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09e48d1a-87ed-4b61-9049-21dd06e649a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "36c04a86-dac4-4fe1-ab6f-bcb6f8ed0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path\n",
    "path = '../documents/processed_docs/pandas_docs.pkl'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Save data to a file\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(pandas_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c62cf-5458-4f65-ae1f-e4dd658d5f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16a01e5-d5ae-4b0b-813c-36a41927b982",
   "metadata": {
    "tags": []
   },
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8126cc-4a01-4096-bc96-257cbe7f6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_html_file(html_file):\n",
    "    loader = BSHTMLLoader(html_file, open_encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    \n",
    "    for doc in data:\n",
    "        if 'soup' in doc.metadata:\n",
    "            soup = doc.metadata['soup']\n",
    "        else:\n",
    "            with open(html_file, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "        \n",
    "        target_main = soup.find('div', class_='bd-article-container')\n",
    "        if target_main:\n",
    "            # Remove 'admonition seealso' div, 'prev-next-footer' footer, and 'header-article-items header-article__inner' div\n",
    "            for element in target_main.find_all(['div', 'footer', 'a', 'p'], \n",
    "                                                class_=['admonition seealso','footer-article-item',\n",
    "                                                        'prev-next-footer',\n",
    "                                                        'header-article-items header-article__inner',\n",
    "                                                       'sphx-glr-timing']):\n",
    "                element.decompose()\n",
    "\n",
    "            # Extract and format the text content\n",
    "            formatted_text = []\n",
    "            seen_content = set()  # To keep track of unique content\n",
    "            for element in target_main.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'pre', 'code', 'dt']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if text and text not in seen_content:\n",
    "                    if element.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'dt']:\n",
    "                        formatted_text.append(f'\\n{text}\\n')\n",
    "                    elif element.name == 'pre':\n",
    "                        formatted_text.append(f'\\n{element.get_text()}\\n')\n",
    "                    elif element.name == 'code':\n",
    "                        formatted_text.append(text)\n",
    "                    seen_content.add(text)\n",
    "            \n",
    "            doc.page_content = ''.join(formatted_text).strip()\n",
    "        else:\n",
    "            doc.page_content = \"No content found in the specified main tag.\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parallel_load(html_files):\n",
    "  with multiprocessing.Pool() as pool:\n",
    "    results = list(tqdm(pool.imap(process_html_file, html_files), total=len(html_files), desc=\"Loading files\"))\n",
    "    return [item for sublist in results for item in sublist]  # Flatten the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea2ee7d-9bbe-434b-9ec1-0d775c3f5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_learn_file_path = \"../documents/scikit_learn_docs/\"\n",
    "scikit_learn_include_folders = ['auto_examples', 'computing', \n",
    "                               'datasets', 'modules', 'developers', 'whats_new', 'notebooks']\n",
    "scikit_learn_html_files = load_html_files(scikit_learn_file_path, scikit_learn_include_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0cf2c3-b353-420c-a153-8b7488414954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scikit_learn_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326baba1-7158-489a-8500-63e14c9ce690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for i in range(len(scikit_learn_html_files)):\n",
    "#     if 'index' in scikit_learn_html_files[i]:\n",
    "#         c+=1\n",
    "#         print(scikit_learn_html_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f19b50d-5b6f-4121-bc76-449de5cb25ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 948/948 [00:04<00:00, 227.93it/s]\n"
     ]
    }
   ],
   "source": [
    "scikit_learn_data = parallel_load(scikit_learn_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef711aa9-09f9-4dc3-ae19-5a9d8380d1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scikit_learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f11969-573f-4cbd-9a18-25a77578724e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32434f9-6cb5-404c-b596-a3e56aa69258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the old and new base URLs\n",
    "scikit_learn_old_base_url = \"../documents/scikit_learn_docs\"\n",
    "scikit_learn_new_base_url = \"https://scikit-learn.org/stable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ef6a2c-dd94-4ff1-b0d0-af534ae804f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each document and update the source in the metadata\n",
    "for doc in scikit_learn_data:\n",
    "    if 'source' in doc.metadata:\n",
    "        doc.metadata['source'] = doc.metadata['source'].replace(scikit_learn_old_base_url, scikit_learn_new_base_url)\n",
    "\n",
    "# Now, pandas_docs contains updated sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8c4a1121-08cf-4bf7-b3d4-84730eb9849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='6.6.Random Projection#\n",
      "\n",
      "Thesklearn.random_projectionmodule implements a simple and\n",
      "computationally efficient way to reduce the dimensionality of the data by\n",
      "trading a controlled amount of accuracy (as additional variance) for faster\n",
      "processing times and smaller model sizes. This module implements two types of\n",
      "unstructured random matrix:Gaussian random matrixandsparse random matrix.\n",
      "sklearn.random_projection\n",
      "The dimensions and distribution of random projections matrices are\n",
      "controlled so as to preserve the pairwise distances between any two\n",
      "samples of the dataset. Thus random projection is a suitable approximation\n",
      "technique for distance based method.\n",
      "\n",
      "References\n",
      "\n",
      "Sanjoy Dasgupta. 2000.Experiments with random projection.In Proceedings of the Sixteenth conference on Uncertainty in artificial\n",
      "intelligence (UAI’00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan\n",
      "Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.\n",
      "\n",
      "Ella Bingham and Heikki Mannila. 2001.Random projection in dimensionality reduction: applications to image and text data.In Proceedings of the seventh ACM SIGKDD international conference on\n",
      "Knowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA,\n",
      "245-250.\n",
      "\n",
      "6.6.1.The Johnson-Lindenstrauss lemma#\n",
      "\n",
      "The main theoretical result behind the efficiency of random projection is theJohnson-Lindenstrauss lemma (quoting Wikipedia):\n",
      "\n",
      "In mathematics, the Johnson-Lindenstrauss lemma is a result\n",
      "concerning low-distortion embeddings of points from high-dimensional\n",
      "into low-dimensional Euclidean space. The lemma states that a small set\n",
      "of points in a high-dimensional space can be embedded into a space of\n",
      "much lower dimension in such a way that distances between the points are\n",
      "nearly preserved. The map used for the embedding is at least Lipschitz,\n",
      "and can even be taken to be an orthogonal projection.\n",
      "\n",
      "Knowing only the number of samples, thejohnson_lindenstrauss_min_dimestimates\n",
      "conservatively the minimal size of the random subspace to guarantee a\n",
      "bounded distortion introduced by the random projection:\n",
      "johnson_lindenstrauss_min_dim\n",
      ">>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
      ">>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\n",
      "663\n",
      ">>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\n",
      "array([    663,   11841, 1112658])\n",
      ">>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\n",
      "array([ 7894,  9868, 11841])\n",
      "\n",
      "\n",
      "Examples\n",
      "\n",
      "SeeThe Johnson-Lindenstrauss bound for embedding with random projectionsfor a theoretical explication on the Johnson-Lindenstrauss lemma and an\n",
      "empirical validation using sparse random matrices.\n",
      "\n",
      "Sanjoy Dasgupta and Anupam Gupta, 1999.An elementary proof of the Johnson-Lindenstrauss Lemma.\n",
      "\n",
      "6.6.2.Gaussian random projection#\n",
      "\n",
      "TheGaussianRandomProjectionreduces the\n",
      "dimensionality by projecting the original input space on a randomly generated\n",
      "matrix where components are drawn from the following distribution\\(N(0, \\frac{1}{n_{components}})\\).\n",
      "GaussianRandomProjection\n",
      "Here a small excerpt which illustrates how to use the Gaussian random\n",
      "projection transformer:\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from sklearn import random_projection\n",
      ">>> X = np.random.rand(100, 10000)\n",
      ">>> transformer = random_projection.GaussianRandomProjection()\n",
      ">>> X_new = transformer.fit_transform(X)\n",
      ">>> X_new.shape\n",
      "(100, 3947)\n",
      "\n",
      "\n",
      "6.6.3.Sparse random projection#\n",
      "\n",
      "TheSparseRandomProjectionreduces the\n",
      "dimensionality by projecting the original input space using a sparse\n",
      "random matrix.\n",
      "SparseRandomProjection\n",
      "Sparse random matrices are an alternative to dense Gaussian random\n",
      "projection matrix that guarantees similar embedding quality while being much\n",
      "more memory efficient and allowing faster computation of the projected data.\n",
      "\n",
      "If we defines=1/density, the elements of the random matrix\n",
      "are drawn from\n",
      "s=1/density\n",
      "where\\(n_{\\text{components}}\\)is the size of the projected subspace.\n",
      "By default the density of non zero elements is set to the minimum density as\n",
      "recommended by Ping Li et al.:\\(1 / \\sqrt{n_{\\text{features}}}\\).\n",
      "\n",
      "Here a small excerpt which illustrates how to use the sparse random\n",
      "projection transformer:\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from sklearn import random_projection\n",
      ">>> X = np.random.rand(100, 10000)\n",
      ">>> transformer = random_projection.SparseRandomProjection()\n",
      ">>> X_new = transformer.fit_transform(X)\n",
      ">>> X_new.shape\n",
      "(100, 3947)\n",
      "\n",
      "\n",
      "D. Achlioptas. 2003.Database-friendly random projections: Johnson-Lindenstrauss  with binary\n",
      "coins.\n",
      "Journal of Computer and System Sciences 66 (2003) 671-687.\n",
      "\n",
      "Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.Very sparse random projections.In Proceedings of the 12th ACM SIGKDD international conference on\n",
      "Knowledge discovery and data mining (KDD ‘06). ACM, New York, NY, USA, 287-296.\n",
      "\n",
      "6.6.4.Inverse Transform#\n",
      "\n",
      "The random projection transformers havecompute_inverse_componentsparameter. When\n",
      "set to True, after creating the randomcomponents_matrix during fitting,\n",
      "the transformer computes the pseudo-inverse of this matrix and stores it asinverse_components_. Theinverse_components_matrix has shape\\(n_{features} \\times n_{components}\\), and it is always a dense matrix,\n",
      "regardless of whether the components matrix is sparse or dense. So depending on\n",
      "the number of features and components, it may use a lot of memory.\n",
      "compute_inverse_componentscomponents_inverse_components_\n",
      "When theinverse_transformmethod is called, it computes the product of the\n",
      "inputXand the transpose of the inverse components. If the inverse components have\n",
      "been computed during fit, they are reused at each call toinverse_transform.\n",
      "Otherwise they are recomputed each time, which can be costly. The result is always\n",
      "dense, even ifXis sparse.\n",
      "inverse_transformX\n",
      "Here a small code example which illustrates how to use the inverse transform\n",
      "feature:\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.random_projection import SparseRandomProjection\n",
      ">>> X = np.random.rand(100, 10000)\n",
      ">>> transformer = SparseRandomProjection(\n",
      "...   compute_inverse_components=True\n",
      "... )\n",
      "...\n",
      ">>> X_new = transformer.fit_transform(X)\n",
      ">>> X_new.shape\n",
      "(100, 3947)\n",
      ">>> X_new_inversed = transformer.inverse_transform(X_new)\n",
      ">>> X_new_inversed.shape\n",
      "(100, 10000)\n",
      ">>> X_new_again = transformer.transform(X_new_inversed)\n",
      ">>> np.allclose(X_new, X_new_again)\n",
      "True' metadata={'source': 'https://scikit-learn.org/stable/modules/random_projection.html', 'title': '6.6. Random Projection — scikit-learn 1.5.1 documentation'}\n"
     ]
    }
   ],
   "source": [
    "print(scikit_learn_data[350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d9d6e1ba-39c6-41fa-9ef3-5827c769a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='safe_mask#\n",
      "\n",
      "sklearn.utils.safe_mask(X,mask)[source]#\n",
      "\n",
      "Return a mask which is safe to use on X.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "X{array-like, sparse matrix}\n",
      "\n",
      "Data on which to apply mask.\n",
      "\n",
      "maskarray-like\n",
      "\n",
      "Mask to be used on X.\n",
      "\n",
      "Returns:\n",
      "\n",
      "maskndarray\n",
      "\n",
      "Array that is safe to use on X.\n",
      "\n",
      "Examples\n",
      "\n",
      ">>> from sklearn.utils import safe_mask\n",
      ">>> from scipy.sparse import csr_matrix\n",
      ">>> data = csr_matrix([[1], [2], [3], [4], [5]])\n",
      ">>> condition = [False, True, True, False, True]\n",
      ">>> mask = safe_mask(data, condition)\n",
      ">>> data[mask].toarray()\n",
      "array([[2],\n",
      "       [3],\n",
      "       [5]])' metadata={'source': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.safe_mask.html', 'title': 'safe_mask — scikit-learn 1.5.1 documentation'}\n"
     ]
    }
   ],
   "source": [
    "print(scikit_learn_data[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "a543a3f6-63f5-4677-9467-70e11ba5b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='safe_mask#\n",
      "\n",
      "sklearn.utils.safe_mask(X,mask)[source]#\n",
      "\n",
      "Return a mask which is safe to use on X.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "X{array-like, sparse matrix}\n",
      "\n",
      "Data on which to apply mask.\n",
      "\n",
      "maskarray-like\n",
      "\n",
      "Mask to be used on X.\n",
      "\n",
      "Returns:\n",
      "\n",
      "maskndarray\n",
      "\n",
      "Array that is safe to use on X.\n",
      "\n",
      "Examples\n",
      "\n",
      ">>> from sklearn.utils import safe_mask\n",
      ">>> from scipy.sparse import csr_matrix\n",
      ">>> data = csr_matrix([[1], [2], [3], [4], [5]])\n",
      ">>> condition = [False, True, True, False, True]\n",
      ">>> mask = safe_mask(data, condition)\n",
      ">>> data[mask].toarray()\n",
      "array([[2],\n",
      "       [3],\n",
      "       [5]])' metadata={'source': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.safe_mask.html', 'title': 'safe_mask — scikit-learn 1.5.1 documentation'}\n"
     ]
    }
   ],
   "source": [
    "print(scikit_learn_data[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6faee954-fc63-478e-8b74-0132a0fa2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 81, 88, 115, 121, 144, 191, 213, 290, 313, 338, 347, 507, 514, 839, 896, 928]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_no_content = []\n",
    "for i in range(len(scikit_learn_data)):\n",
    "    if scikit_learn_data[i].page_content == 'No content found in the specified main tag.'  or \\\n",
    "       'This document has been moved' in scikit_learn_data[i].page_content or \\\n",
    "        len(scikit_learn_data[i].page_content) < 200:\n",
    "        docs_with_no_content.append(i)\n",
    "        \n",
    "print(docs_with_no_content)\n",
    "len(docs_with_no_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd8d847f-a20c-423c-9347-9042e95bbd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[654]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_no_content = []\n",
    "for i in range(len(scikit_learn_data)):\n",
    "    if 'Normalizes' in scikit_learn_data[i].page_content:\n",
    "        docs_with_no_content.append(i)\n",
    "        \n",
    "print(docs_with_no_content)\n",
    "len(docs_with_no_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9f2ef09-8f0b-4d0d-932c-c1a759df172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html', 'title': 'confusion_matrix — scikit-learn 1.5.1 documentation'}, page_content='confusion_matrix#\\n\\nsklearn.metrics.confusion_matrix(y_true,y_pred,*,labels=None,sample_weight=None,normalize=None)[source]#\\n\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n\\nBy definition a confusion matrix\\\\(C\\\\)is such that\\\\(C_{i, j}\\\\)is equal to the number of observations known to be in group\\\\(i\\\\)and\\npredicted to be in group\\\\(j\\\\).\\n\\nThus in binary classification, the count of true negatives is\\\\(C_{0,0}\\\\), false negatives is\\\\(C_{1,0}\\\\), true positives is\\\\(C_{1,1}\\\\)and false positives is\\\\(C_{0,1}\\\\).\\n\\nRead more in theUser Guide.\\n\\nParameters:\\n\\ny_truearray-like of shape (n_samples,)\\n\\nGround truth (correct) target values.\\n\\ny_predarray-like of shape (n_samples,)\\n\\nEstimated targets as returned by a classifier.\\n\\nlabelsarray-like of shape (n_classes), default=None\\n\\nList of labels to index the matrix. This may be used to reorder\\nor select a subset of labels.\\nIfNoneis given, those that appear at least once\\niny_trueory_predare used in sorted order.\\nNoney_truey_pred\\nsample_weightarray-like of shape (n_samples,), default=None\\n\\nSample weights.\\n\\nAdded in version 0.18.\\n\\nnormalize{‘true’, ‘pred’, ‘all’}, default=None\\n\\nNormalizes confusion matrix over the true (rows), predicted (columns)\\nconditions or all the population. If None, confusion matrix will not be\\nnormalized.\\n\\nReturns:\\n\\nCndarray of shape (n_classes, n_classes)\\n\\nConfusion matrix whose i-th row and j-th\\ncolumn entry indicates the number of\\nsamples with true label being i-th class\\nand predicted label being j-th class.\\n\\nReferences\\n\\nWikipedia entry for the Confusion matrix(Wikipedia and other references may use a different\\nconvention for axes).\\n\\nExamples\\n\\n>>> from sklearn.metrics import confusion_matrix\\n>>> y_true = [2, 0, 2, 2, 0, 1]\\n>>> y_pred = [0, 0, 2, 2, 0, 2]\\n>>> confusion_matrix(y_true, y_pred)\\narray([[2, 0, 0],\\n       [0, 0, 1],\\n       [1, 0, 2]])\\n\\n\\n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\\n>>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\\narray([[2, 0, 0],\\n       [0, 0, 1],\\n       [1, 0, 2]])\\n\\n\\nIn the binary case, we can extract true positives, etc. as follows:\\n\\n>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\\n>>> (tn, fp, fn, tp)\\n(0, 2, 1, 1)\\n\\n\\nGallery examples#\\n\\nRelease Highlights for scikit-learn 1.5\\n\\nVisualizations with Display Objects\\n\\nPost-tuning the decision threshold for cost-sensitive learning\\n\\nLabel Propagation digits active learning')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scikit_learn_data[654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "18d35a8f-0c92-43f0-85be-f9b6b7256caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the documents with no content\n",
    "# If pandas_data is a list:\n",
    "scikit_learn_data = [doc for i, doc in enumerate(scikit_learn_data) if i not in docs_with_no_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "762a00a3-e42a-49b9-854b-aac62b21a90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scikit_learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1cb74427-abd7-44ba-a251-b3d376c7648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path\n",
    "path = '../documents/processed_docs/scikit_learn_docs.pkl'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Save data to a file\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(scikit_learn_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd3d71-6bef-4429-b7c0-84f37f27f7c3",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cb7497-4966-4a50-9123-1a6adc63b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_html_file(html_file):\n",
    "    loader = BSHTMLLoader(html_file, open_encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    \n",
    "    for doc in data:\n",
    "        if 'soup' in doc.metadata:\n",
    "            soup = doc.metadata['soup']\n",
    "        else:\n",
    "            with open(html_file, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "        \n",
    "        target_main = soup.find('div', class_='bd-article-container')\n",
    "        if target_main:\n",
    "            # Remove 'admonition seealso' div, 'prev-next-footer' footer, and 'header-article-items header-article__inner' div\n",
    "            for element in target_main.find_all(['div', 'footer', 'a', 'p'], \n",
    "                                                class_=['admonition seealso','footer-article-item',\n",
    "                                                        'prev-next-footer',\n",
    "                                                        'header-article-items header-article__inner',\n",
    "                                                       'sphx-glr-timing']):\n",
    "                element.decompose()\n",
    "\n",
    "            # Extract and format the text content\n",
    "            formatted_text = []\n",
    "            seen_content = set()  # To keep track of unique content\n",
    "            for element in target_main.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'pre', 'code', 'dt']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if text and text not in seen_content:\n",
    "                    if element.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'dt']:\n",
    "                        formatted_text.append(f'\\n{text}\\n')\n",
    "                    elif element.name == 'pre':\n",
    "                        formatted_text.append(f'\\n{element.get_text()}\\n')\n",
    "                    elif element.name == 'code':\n",
    "                        formatted_text.append(text)\n",
    "                    seen_content.add(text)\n",
    "            \n",
    "            doc.page_content = ''.join(formatted_text).strip()\n",
    "        else:\n",
    "            doc.page_content = \"No content found in the specified main tag.\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parallel_load(html_files):\n",
    "  with multiprocessing.Pool() as pool:\n",
    "    results = list(tqdm(pool.imap(process_html_file, html_files), total=len(html_files), desc=\"Loading files\"))\n",
    "    return [item for sublist in results for item in sublist]  # Flatten the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c5c92e-ee07-4d44-8c31-35c239439cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2699"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_file_path = \"../documents/numpy_docs/\"\n",
    "numpy_include_folders = ['user', 'reference', \n",
    "                               'building', 'dev', 'f2py', 'release']b\n",
    "numpy_html_files = load_html_files(numpy_file_path, numpy_include_folders)\n",
    "len(numpy_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480a44a9-501f-4a9f-8081-0e6193f6e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for i in range(len(numpy_html_files)):\n",
    "#     if 'index' in numpy_html_files[i]:\n",
    "#         c+=1\n",
    "#         print(numpy_html_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e2c471-a094-491d-95d5-e7cb8cdef074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 2699/2699 [00:04<00:00, 659.01it/s]\n"
     ]
    }
   ],
   "source": [
    "numpy_data = parallel_load(numpy_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742aa7ff-d1f3-4eff-902f-57814a8fc8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2699"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numpy_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d881d584-828b-4587-8036-c5aa17ae60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the old and new base URLs\n",
    "numpy_old_base_url = \"../documents/numpy_docs\"\n",
    "numpy_new_base_url = \"https://numpy.org/doc/stable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6549978a-b154-4dd4-8796-531d2352977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each document and update the source in the metadata\n",
    "for doc in numpy_data:\n",
    "    if 'source' in doc.metadata:\n",
    "        doc.metadata['source'] = doc.metadata['source'].replace(numpy_old_base_url, numpy_new_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7115e196-95dd-4af9-834b-b335d32f4c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='NumPy C code explanations#\n",
      "\n",
      "This document has been moved toNumPy C code explanations.' metadata={'source': 'https://numpy.org/doc/stable/reference/internals.code-explanations.html', 'title': 'NumPy C code explanations — NumPy v2.0 Manual'}\n"
     ]
    }
   ],
   "source": [
    "print(numpy_data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72290667-b206-44a5-be7d-9e23e005accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='NumPy internals#\n",
      "\n",
      "This document has been moved toInternal organization of NumPy arrays.' metadata={'source': 'https://numpy.org/doc/stable/reference/internals.html', 'title': 'NumPy internals — NumPy v2.0 Manual'}\n"
     ]
    }
   ],
   "source": [
    "print(numpy_data[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "717b053e-75e3-49a3-8f4c-57eff7e2586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_no_content = []\n",
    "for i in range(len(numpy_data)):\n",
    "    if numpy_data[i].page_content == 'No content found in the specified main tag.':\n",
    "        docs_with_no_content.append(i)\n",
    "        \n",
    "print(docs_with_no_content)\n",
    "len(docs_with_no_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9e8b958-6d9e-43ff-94a4-02409302810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 31, 34, 50, 51, 232, 234, 260, 272, 276, 309, 470, 475, 477, 478, 487, 488, 489, 490, 491, 492, 514, 516, 532, 602, 603, 605, 606, 609, 610, 611, 612, 613, 614, 615, 616, 618, 619, 621, 645, 646, 697, 715, 716, 722, 728, 729, 730, 732, 733, 792, 797, 799, 803, 804, 807, 808, 810, 811, 815, 816, 817, 820, 823, 831, 832, 840, 849, 851, 853, 855, 859, 863, 865, 868, 870, 874, 876, 877, 882, 883, 884, 885, 886, 887, 891, 894, 900, 902, 905, 908, 912, 915, 969, 1108, 1126, 1131, 1134, 1140, 1155, 1170, 1234, 1279, 1281, 1298, 1306, 1330, 1365, 1367, 1380, 1388, 1392, 1412, 1447, 1448, 1449, 1452, 1454, 1455, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1497, 1498, 1499, 1500, 1520, 1522, 1534, 1542, 1546, 1566, 1582, 1584, 1585, 1586, 1587, 1589, 1590, 1591, 1593, 1594, 1595, 1596, 1597, 1598, 1602, 1603, 1604, 1623, 1624, 1625, 1626, 1627, 1631, 1632, 1633, 1635, 1705, 1724, 1726, 1838, 1840, 1895, 1897, 2039, 2041, 2045, 2054, 2062, 2066, 2086, 2103, 2108, 2113, 2115, 2119, 2120, 2124, 2126, 2127, 2131, 2132, 2133, 2135, 2137, 2140, 2148, 2149, 2153, 2157, 2304, 2363, 2367, 2394, 2397, 2398, 2399, 2401, 2402]\n",
      "235\n"
     ]
    }
   ],
   "source": [
    "docs_with_no_content = []\n",
    "for i in range(len(numpy_data)):\n",
    "    if numpy_data[i].page_content == 'No content found in the specified main tag.' or \\\n",
    "       'This document has been moved' in numpy_data[i].page_content or \\\n",
    "        len(numpy_data[i].page_content) < 100:\n",
    "        docs_with_no_content.append(i)\n",
    "\n",
    "print(docs_with_no_content)\n",
    "print(len(docs_with_no_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bf27d7e-42b1-4fa0-9a33-1df18ebb4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the documents with no content\n",
    "# If pandas_data is a list:\n",
    "numpy_data = [doc for i, doc in enumerate(numpy_data) if i not in docs_with_no_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a919371-fbbc-4a35-9226-4b08d2a3fb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2464"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "baf90917-cc7a-4f60-961b-67d59a4ee097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path\n",
    "path = '../documents/processed_docs/numpy_docs.pkl'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Save data to a file\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(numpy_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f8d37-b9bc-4596-8aa1-7eac747fe41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bf79d9a-261f-442b-b250-f8d887de4557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='numpy.finfo#\n",
      "\n",
      "classnumpy.finfo(dtype)[source]#\n",
      "\n",
      "Machine limits for floating point types.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "dtypefloat, dtype, or instance\n",
      "\n",
      "Kind of floating point or complex floating point\n",
      "data-type about which to get information.\n",
      "\n",
      "Notes\n",
      "\n",
      "For developers of NumPy: do not instantiate this at the module level.\n",
      "The initial calculation of these parameters is expensive and negatively\n",
      "impacts import times.  These objects are cached, so callingfinfo()repeatedly inside your functions is not a problem.\n",
      "finfo()\n",
      "Note thatsmallest_normalis not actually the smallest positive\n",
      "representable value in a NumPy floating point type. As in the IEEE-754\n",
      "standard[1], NumPy floating point types make use of subnormal numbers to\n",
      "fill the gap between 0 andsmallest_normal. However, subnormal numbers\n",
      "may have significantly reduced precision[2].\n",
      "smallest_normal\n",
      "This function can also be used for complex data types as well. If used,\n",
      "the output will be the same as the corresponding real float type\n",
      "(e.g. numpy.finfo(numpy.csingle) is the same as numpy.finfo(numpy.single)).\n",
      "However, the output is true for the real and imaginary components.\n",
      "\n",
      "References\n",
      "\n",
      "IEEE Standard for Floating-Point Arithmetic, IEEE Std 754-2008,\n",
      "pp.1-70, 2008,https://doi.org/10.1109/IEEESTD.2008.4610935\n",
      "\n",
      "Wikipedia, “Denormal Numbers”,https://en.wikipedia.org/wiki/Denormal_number\n",
      "\n",
      "Examples\n",
      "\n",
      ">>> np.finfo(np.float64).dtype\n",
      "dtype('float64')\n",
      ">>> np.finfo(np.complex64).dtype\n",
      "dtype('float32')\n",
      "\n",
      "\n",
      "Attributes:\n",
      "\n",
      "bitsint\n",
      "\n",
      "The number of bits occupied by the type.\n",
      "\n",
      "dtypedtype\n",
      "\n",
      "Returns the dtype for whichfinforeturns information. For complex\n",
      "input, the returned dtype is the associatedfloat*dtype for its\n",
      "real and complex components.\n",
      "finfofloat*\n",
      "epsfloat\n",
      "\n",
      "The difference between 1.0 and the next smallest representable float\n",
      "larger than 1.0. For example, for 64-bit binary floats in the IEEE-754\n",
      "standard,eps=2**-52, approximately 2.22e-16.\n",
      "eps=2**-52\n",
      "epsnegfloat\n",
      "\n",
      "The difference between 1.0 and the next smallest representable float\n",
      "less than 1.0. For example, for 64-bit binary floats in the IEEE-754\n",
      "standard,epsneg=2**-53, approximately 1.11e-16.\n",
      "epsneg=2**-53\n",
      "iexpint\n",
      "\n",
      "The number of bits in the exponent portion of the floating point\n",
      "representation.\n",
      "\n",
      "machepint\n",
      "\n",
      "The exponent that yieldseps.\n",
      "\n",
      "maxfloating point number of the appropriate type\n",
      "\n",
      "The largest representable number.\n",
      "\n",
      "maxexpint\n",
      "\n",
      "The smallest positive power of the base (2) that causes overflow.\n",
      "\n",
      "minfloating point number of the appropriate type\n",
      "\n",
      "The smallest representable number, typically-max.\n",
      "-max\n",
      "minexpint\n",
      "\n",
      "The most negative power of the base (2) consistent with there\n",
      "being no leading 0’s in the mantissa.\n",
      "\n",
      "negepint\n",
      "\n",
      "The exponent that yieldsepsneg.\n",
      "\n",
      "nexpint\n",
      "\n",
      "The number of bits in the exponent including its sign and bias.\n",
      "\n",
      "nmantint\n",
      "\n",
      "The number of bits in the mantissa.\n",
      "\n",
      "precisionint\n",
      "\n",
      "The approximate number of decimal digits to which this kind of\n",
      "float is precise.\n",
      "\n",
      "resolutionfloating point number of the appropriate type\n",
      "\n",
      "The approximate decimal resolution of this type, i.e.,10**-precision.\n",
      "10**-precision\n",
      "tinyfloat\n",
      "tiny\n",
      "Return the value for tiny, alias of smallest_normal.\n",
      "\n",
      "smallest_normalfloat\n",
      "\n",
      "Return the value for the smallest normal.\n",
      "\n",
      "smallest_subnormalfloat\n",
      "\n",
      "The smallest positive floating point number with 0 as leading bit in\n",
      "the mantissa following IEEE-754.' metadata={'source': 'https://numpy.org/doc/stable/reference/generated/numpy.finfo.html', 'title': 'numpy.finfo — NumPy v2.0 Manual'}\n"
     ]
    }
   ],
   "source": [
    "print(numpy_data[542])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32481e34-9ab0-4d8e-ba43-4ff27a26c97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
