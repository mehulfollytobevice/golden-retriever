{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6c113f-30f1-45c5-88cc-8512335403ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrawal.bi/.local/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-07-28 21:28:23.795167: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 21:28:23.795212: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 21:28:23.795254: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 21:28:23.808148: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 21:28:25.615610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import time\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "import shutil\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain.storage import LocalFileStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd6267-b31f-40b6-b9d2-630233cf4ca4",
   "metadata": {},
   "source": [
    "## Reading saved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f06565-fdd4-4f5f-8113-1897ac39643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path\n",
    "path = '../documents/processed_docs/pandas_docs.pkl'\n",
    "\n",
    "# Load data from a file\n",
    "with open(path, 'rb') as file:\n",
    "    pandas_docs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc047a00-85fa-41d1-b6aa-39bf4cf2e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pandas_docs[134].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db1f901-fb7e-4b09-af10-b3ee4a14eb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html', 'title': 'pandas.DataFrame.idxmax — pandas 2.2.2 documentation'}, page_content='pandas.DataFrame.idxmax#\\n\\nDataFrame.idxmax(axis=0,skipna=True,numeric_only=False)[source]#\\n\\nReturn index of first occurrence of maximum over requested axis.\\n\\nNA/null values are excluded.\\n\\nParameters:\\n\\naxis{0 or ‘index’, 1 or ‘columns’}, default 0\\n\\nThe axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise.\\n\\nskipnabool, default True\\n\\nExclude NA/null values. If an entire row/column is NA, the result\\nwill be NA.\\n\\nnumeric_onlybool, default False\\n\\nInclude onlyfloat,intorbooleandata.\\n\\nNew in version 1.5.0.\\n\\nReturns:\\n\\nSeries\\n\\nIndexes of maxima along the specified axis.\\n\\nRaises:\\n\\nValueError\\n\\nIf the row/column is empty\\n\\nNotes\\n\\nThis method is the DataFrame version ofndarray.argmax.\\nndarray.argmax\\nExamples\\n\\nConsider a dataset containing food consumption in Argentina.\\n\\n>>> df = pd.DataFrame({\\'consumption\\': [10.51, 103.11, 55.48],\\n...                     \\'co2_emissions\\': [37.2, 19.66, 1712]},\\n...                   index=[\\'Pork\\', \\'Wheat Products\\', \\'Beef\\'])\\n\\n\\n>>> df\\n                consumption  co2_emissions\\nPork                  10.51         37.20\\nWheat Products       103.11         19.66\\nBeef                  55.48       1712.00\\n\\n\\nBy default, it returns the index for the maximum value in each column.\\n\\n>>> df.idxmax()\\nconsumption     Wheat Products\\nco2_emissions             Beef\\ndtype: object\\n\\n\\nTo return the index for the maximum value in each row, useaxis=\"columns\".\\naxis=\"columns\"\\n>>> df.idxmax(axis=\"columns\")\\nPork              co2_emissions\\nWheat Products     consumption\\nBeef              co2_emissions\\ndtype: object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_docs[134]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0ca19-a6ec-4509-904a-6fd96252c43b",
   "metadata": {},
   "source": [
    "## Embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca729f4-47a0-460f-98b8-7bcfcea3fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/parent_document_retriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b713d89-8a8f-4775-867c-014e21381b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to update this\n",
    "# !pip install --upgrade pyOpenSSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a5b1f0-c038-49a5-9143-25f3c1d745af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 MB\n",
      "Free Memory: 34079.90 MB\n",
      "Total Memory: 34079.90 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def memory_check():\n",
    "    # Get the memory allocated on the GPU\n",
    "    allocated_memory = torch.cuda.memory_allocated()\n",
    "    # Get the total memory on the GPU\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    # Get the free memory\n",
    "    free_memory = total_memory - allocated_memory\n",
    "\n",
    "    print(f\"Allocated Memory: {allocated_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Free Memory: {free_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Total Memory: {total_memory / 1e6:.2f} MB\")\n",
    "    \n",
    "memory_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e821e-f6db-4a2b-95f5-ae6a8914ecff",
   "metadata": {},
   "source": [
    "## embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d39c2eb-54f0-4241-a568-ae24b7b6aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"thenlper/gte-large\"\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=model_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5903e7-e6a3-4ab0-9f01-9d2ea89b6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29dae36a-2cde-4432-9e76-b518539db020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 1745.80 MB\n",
      "Free Memory: 32334.10 MB\n",
      "Total Memory: 34079.90 MB\n"
     ]
    }
   ],
   "source": [
    "memory_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0dd7126-9a98-4880-afb2-536995048508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://huggingface.co/dunzhang/stella_en_400M_v5\n",
    "\n",
    "\n",
    "# model_name = \"dunzhang/stella_en_400M_v5\"\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs={\"trust_remote_code\": True}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46b024-7254-48d8-a96f-037ca56e2414",
   "metadata": {},
   "source": [
    "### Using ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02238cec-d166-4ec6-a275-167182083f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_doc_path = \"/home/agrawal.bi/DS5500/vector_db/parent_docs\"\n",
    "child_doc_path = \"../vector_db/child_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c85bad7-323b-473a-bb88-6fcb6349a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders emptied successfully.\n"
     ]
    }
   ],
   "source": [
    "# Paths to empty\n",
    "parent_docs_path = parent_doc_path\n",
    "child_docs_path = os.path.abspath(child_doc_path)\n",
    "\n",
    "# Function to empty a folder\n",
    "def empty_folder(folder):\n",
    "    for item in os.listdir(folder):\n",
    "        item_path = os.path.join(folder, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {item_path}. Reason: {e}')\n",
    "\n",
    "# Empty the folders\n",
    "empty_folder(parent_docs_path)\n",
    "empty_folder(child_docs_path)\n",
    "\n",
    "print(\"Folders emptied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb902af9-6595-4941-9b9b-ae99f0256fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# The vectorstore to use to index the child chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16217eec-a3a3-437f-898c-c705c1e71418",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = LocalFileStore(parent_doc_path)\n",
    "store = create_kv_docstore(fs)\n",
    "\n",
    "vectorstore = Chroma(collection_name= \"split_parents\", \n",
    "                     embedding_function= embeddings, \n",
    "                     persist_directory= child_doc_path)\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c73f78-1090-4f11-9fd3-c2ea5f57a38f",
   "metadata": {},
   "source": [
    "### adding pandas docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdbcf5b7-3f01-4336-99b1-080e88cc9243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 2286/2286 [07:40<00:00,  4.96doc/s]\n"
     ]
    }
   ],
   "source": [
    "# Adding docs to local storage\n",
    "for doc in tqdm(pandas_docs, desc=\"Adding documents\", unit=\"doc\"):\n",
    "    retriever.add_documents([doc], ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af75f25c-51d2-4a9f-bce7-656f3e265535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chroma vectorstore\n",
    "loaded_vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory= child_doc_path\n",
    ")\n",
    " \n",
    "# Load parent document store\n",
    "loaded_file_store = LocalFileStore(parent_doc_path)\n",
    " \n",
    "# Recreate ParentDocumentRetriever\n",
    "loaded_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=loaded_vectorstore,\n",
    "    docstore=loaded_file_store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae579bec-5ef1-42df-a8fd-5cf6bd8c5cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5135"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(loaded_file_store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20cd2cda-f82e-411b-bc02-4fba2e8767be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '2e013833-a3d8-424d-a721-3ad2da85805c', 'source': 'https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.fillna.html', 'title': 'pandas.core.resample.Resampler.fillna — pandas 2.2.2 documentation'}, page_content='Method to use for filling holes in resampled data\\n\\n‘pad’ or ‘ffill’: use previous valid observation to fill gap\\n(forward fill).\\n\\n‘backfill’ or ‘bfill’: use next valid observation to fill gap.\\n\\n‘nearest’: use nearest valid observation to fill gap.\\n\\nlimitint, optional\\n\\nLimit of how many consecutive missing values to fill.\\n\\nReturns:\\n\\nSeries or DataFrame\\n\\nAn upsampled Series or DataFrame with missing values filled.\\n\\nReferences\\n\\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\\n\\nExamples'),\n",
       " Document(metadata={'doc_id': '34348068-f5e9-4a70-b2ef-a8ced5726c33', 'source': 'https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.interpolate.html', 'title': 'pandas.core.resample.Resampler.interpolate — pandas 2.2.2 documentation'}, page_content='‘time’: Works on daily and higher resolution data to interpolate\\ngiven length of interval.\\n\\n‘index’, ‘values’: use the actual numerical values of the index.\\n\\n‘pad’: Fill in NaNs using existing values.'),\n",
       " Document(metadata={'doc_id': 'b2b20793-ef97-4805-bc30-da3754b49df8', 'source': 'https://pandas.pydata.org/docs/user_guide/basics.html', 'title': 'Essential basic functionality — pandas 2.2.2 documentation'}, page_content='If specified,filldata for missing labels using logic (highly relevant\\nto working with time series data)\\n\\nHere is a simple example:\\n\\nIn [203]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\\n\\nIn [204]: s\\nOut[204]: \\na    1.695148\\nb    1.328614\\nc    1.234686\\nd   -0.385845\\ne   -1.326508\\ndtype: float64\\n\\nIn [205]: s.reindex([\"e\", \"b\", \"f\", \"d\"])\\nOut[205]: \\ne   -1.326508\\nb    1.328614\\nf         NaN\\nd   -0.385845\\ndtype: float64'),\n",
       " Document(metadata={'doc_id': '7027a379-081d-4012-ba0d-94e58578eb9f', 'source': 'https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.bfill.html', 'title': 'pandas.core.resample.Resampler.bfill — pandas 2.2.2 documentation'}, page_content=\"Limit of how many values to fill.\\n\\nReturns:\\n\\nSeries, DataFrame\\n\\nAn upsampled Series or DataFrame with backward filled NaN values.\\n\\nReferences\\n\\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\\n\\nExamples\\n\\nResampling a Series:\\n\\n>>> s = pd.Series([1, 2, 3],\\n...               index=pd.date_range('20180101', periods=3, freq='h'))\\n>>> s\\n2018-01-01 00:00:00    1\\n2018-01-01 01:00:00    2\\n2018-01-01 02:00:00    3\\nFreq: h, dtype: int64\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vectorstore.similarity_search(\"how can I handle irregularly spaced time series data, including filling missing values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5baa75ca-a551-41b1-8626-98e2d16a9840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.fillna.html\", \"title\": \"pandas.core.resample.Resampler.fillna \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"pandas.core.resample.Resampler.fillna#\\\\n\\\\nfinalResampler.fillna(method,limit=None)[source]#\\\\n\\\\nFill missing values introduced by upsampling.\\\\n\\\\nIn statistics, imputation is the process of replacing missing data with\\\\nsubstituted values[1]. When resampling data, missing values may\\\\nappear (e.g., when the resampling frequency is higher than the original\\\\nfrequency).\\\\n\\\\nMissing values that existed in the original data will\\\\nnot be modified.\\\\n\\\\nParameters:\\\\n\\\\nmethod{\\\\u2018pad\\\\u2019, \\\\u2018backfill\\\\u2019, \\\\u2018ffill\\\\u2019, \\\\u2018bfill\\\\u2019, \\\\u2018nearest\\\\u2019}\\\\n\\\\nMethod to use for filling holes in resampled data\\\\n\\\\n\\\\u2018pad\\\\u2019 or \\\\u2018ffill\\\\u2019: use previous valid observation to fill gap\\\\n(forward fill).\\\\n\\\\n\\\\u2018backfill\\\\u2019 or \\\\u2018bfill\\\\u2019: use next valid observation to fill gap.\\\\n\\\\n\\\\u2018nearest\\\\u2019: use nearest valid observation to fill gap.\\\\n\\\\nlimitint, optional\\\\n\\\\nLimit of how many consecutive missing values to fill.\\\\n\\\\nReturns:\\\\n\\\\nSeries or DataFrame\\\\n\\\\nAn upsampled Series or DataFrame with missing values filled.\\\\n\\\\nReferences\\\\n\\\\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\\\\n\\\\nExamples\\\\n\\\\nResampling a Series:\\\\n\\\\n>>> s = pd.Series([1, 2, 3],\\\\n...               index=pd.date_range(\\'20180101\\', periods=3, freq=\\'h\\'))\\\\n>>> s\\\\n2018-01-01 00:00:00    1\\\\n2018-01-01 01:00:00    2\\\\n2018-01-01 02:00:00    3\\\\nFreq: h, dtype: int64\\\\n\\\\n\\\\nWithout filling the missing values you get:\\\\n\\\\n>>> s.resample(\\\\\"30min\\\\\").asfreq()\\\\n2018-01-01 00:00:00    1.0\\\\n2018-01-01 00:30:00    NaN\\\\n2018-01-01 01:00:00    2.0\\\\n2018-01-01 01:30:00    NaN\\\\n2018-01-01 02:00:00    3.0\\\\nFreq: 30min, dtype: float64\\\\n\\\\n\\\\n>>> s.resample(\\'30min\\').fillna(\\\\\"backfill\\\\\")\\\\n2018-01-01 00:00:00    1\\\\n2018-01-01 00:30:00    2\\\\n2018-01-01 01:00:00    2\\\\n2018-01-01 01:30:00    3\\\\n2018-01-01 02:00:00    3\\\\nFreq: 30min, dtype: int64\\\\n\\\\n\\\\n>>> s.resample(\\'15min\\').fillna(\\\\\"backfill\\\\\", limit=2)\\\\n2018-01-01 00:00:00    1.0\\\\n2018-01-01 00:15:00    NaN\\\\n2018-01-01 00:30:00    2.0\\\\n2018-01-01 00:45:00    2.0\\\\n2018-01-01 01:00:00    2.0\\\\n2018-01-01 01:15:00    NaN\\\\n2018-01-01 01:30:00    3.0\\\\n2018-01-01 01:45:00    3.0\\\\n2018-01-01 02:00:00    3.0\\\\nFreq: 15min, dtype: float64\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.interpolate.html\", \"title\": \"pandas.core.resample.Resampler.interpolate \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"pandas.core.resample.Resampler.interpolate#\\\\n\\\\nfinalResampler.interpolate(method=\\'linear\\',*,axis=0,limit=None,inplace=False,limit_direction=\\'forward\\',limit_area=None,downcast=_NoDefault.no_default,**kwargs)[source]#\\\\n\\\\nInterpolate values between target timestamps according to different methods.\\\\n\\\\nThe original index is first reindexed to target timestamps\\\\n(seecore.resample.Resampler.asfreq()),\\\\nthen the interpolation ofNaNvalues viaDataFrame.interpolate()happens.\\\\ncore.resample.Resampler.asfreq()NaNDataFrame.interpolate()\\\\nParameters:\\\\n\\\\nmethodstr, default \\\\u2018linear\\\\u2019\\\\n\\\\nInterpolation technique to use. One of:\\\\n\\\\n\\\\u2018linear\\\\u2019: Ignore the index and treat the values as equally\\\\nspaced. This is the only method supported on MultiIndexes.\\\\n\\\\n\\\\u2018time\\\\u2019: Works on daily and higher resolution data to interpolate\\\\ngiven length of interval.\\\\n\\\\n\\\\u2018index\\\\u2019, \\\\u2018values\\\\u2019: use the actual numerical values of the index.\\\\n\\\\n\\\\u2018pad\\\\u2019: Fill in NaNs using existing values.\\\\n\\\\n\\\\u2018nearest\\\\u2019, \\\\u2018zero\\\\u2019, \\\\u2018slinear\\\\u2019, \\\\u2018quadratic\\\\u2019, \\\\u2018cubic\\\\u2019,\\\\n\\\\u2018barycentric\\\\u2019, \\\\u2018polynomial\\\\u2019: Passed toscipy.interpolate.interp1d, whereas \\\\u2018spline\\\\u2019 is passed toscipy.interpolate.UnivariateSpline. These methods use the numerical\\\\nvalues of the index.  Both \\\\u2018polynomial\\\\u2019 and \\\\u2018spline\\\\u2019 require that\\\\nyou also specify anorder(int), e.g.df.interpolate(method=\\'polynomial\\',order=5). Note that,slinearmethod in Pandas refers to the Scipy first ordersplineinstead of Pandas first orderspline.\\\\ndf.interpolate(method=\\'polynomial\\',order=5)\\\\n\\\\u2018krogh\\\\u2019, \\\\u2018piecewise_polynomial\\\\u2019, \\\\u2018spline\\\\u2019, \\\\u2018pchip\\\\u2019, \\\\u2018akima\\\\u2019,\\\\n\\\\u2018cubicspline\\\\u2019: Wrappers around the SciPy interpolation methods of\\\\nsimilar names. SeeNotes.\\\\n\\\\n\\\\u2018from_derivatives\\\\u2019: Refers toscipy.interpolate.BPoly.from_derivatives.\\\\n\\\\naxis{{0 or \\\\u2018index\\\\u2019, 1 or \\\\u2018columns\\\\u2019, None}}, default None\\\\n\\\\nAxis to interpolate along. ForSeriesthis parameter is unused\\\\nand defaults to 0.\\\\n\\\\nlimitint, optional\\\\n\\\\nMaximum number of consecutive NaNs to fill. Must be greater than\\\\n0.\\\\n\\\\ninplacebool, default False\\\\n\\\\nUpdate the data in place if possible.\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/user_guide/basics.html\", \"title\": \"Essential basic functionality \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"In [198]: df4.map(f)\\\\nOut[198]: \\\\n   one  two  three\\\\na   18   17      3\\\\nb   19   18     20\\\\nc   18   18     16\\\\nd    3   19     19\\\\n\\\\n\\\\nSeries.map()has an additional feature; it can be used to easily\\\\n\\\\u201clink\\\\u201d or \\\\u201cmap\\\\u201d values defined by a secondary series. This is closely related\\\\ntomerging/joining functionality:\\\\nSeries.map()\\\\nIn [199]: s = pd.Series(\\\\n   .....:     [\\\\\"six\\\\\", \\\\\"seven\\\\\", \\\\\"six\\\\\", \\\\\"seven\\\\\", \\\\\"six\\\\\"], index=[\\\\\"a\\\\\", \\\\\"b\\\\\", \\\\\"c\\\\\", \\\\\"d\\\\\", \\\\\"e\\\\\"]\\\\n   .....: )\\\\n   .....: \\\\n\\\\nIn [200]: t = pd.Series({\\\\\"six\\\\\": 6.0, \\\\\"seven\\\\\": 7.0})\\\\n\\\\nIn [201]: s\\\\nOut[201]: \\\\na      six\\\\nb    seven\\\\nc      six\\\\nd    seven\\\\ne      six\\\\ndtype: object\\\\n\\\\nIn [202]: s.map(t)\\\\nOut[202]: \\\\na    6.0\\\\nb    7.0\\\\nc    6.0\\\\nd    7.0\\\\ne    6.0\\\\ndtype: float64\\\\n\\\\n\\\\nReindexing and altering labels#\\\\n\\\\nreindex()is the fundamental data alignment method in pandas.\\\\nIt is used to implement nearly all other features relying on label-alignment\\\\nfunctionality. Toreindexmeans to conform the data to match a given set of\\\\nlabels along a particular axis. This accomplishes several things:\\\\nreindex()\\\\nReorders the existing data to match a new set of labels\\\\n\\\\nInserts missing value (NA) markers in label locations where no data for\\\\nthat label existed\\\\n\\\\nIf specified,filldata for missing labels using logic (highly relevant\\\\nto working with time series data)\\\\n\\\\nHere is a simple example:\\\\n\\\\nIn [203]: s = pd.Series(np.random.randn(5), index=[\\\\\"a\\\\\", \\\\\"b\\\\\", \\\\\"c\\\\\", \\\\\"d\\\\\", \\\\\"e\\\\\"])\\\\n\\\\nIn [204]: s\\\\nOut[204]: \\\\na    1.695148\\\\nb    1.328614\\\\nc    1.234686\\\\nd   -0.385845\\\\ne   -1.326508\\\\ndtype: float64\\\\n\\\\nIn [205]: s.reindex([\\\\\"e\\\\\", \\\\\"b\\\\\", \\\\\"f\\\\\", \\\\\"d\\\\\"])\\\\nOut[205]: \\\\ne   -1.326508\\\\nb    1.328614\\\\nf         NaN\\\\nd   -0.385845\\\\ndtype: float64\\\\n\\\\n\\\\nHere, theflabel was not contained in the Series and hence appears asNaNin the result.\\\\nf\\\\nWith a DataFrame, you can simultaneously reindex the index and columns:\\\\n\\\\nIn [206]: df\\\\nOut[206]: \\\\n        one       two     three\\\\na  1.394981  1.772517       NaN\\\\nb  0.343054  1.912123 -0.050390\\\\nc  0.695246  1.478369  1.227435\\\\nd       NaN  0.279344 -0.613172\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.bfill.html\", \"title\": \"pandas.core.resample.Resampler.bfill \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"pandas.core.resample.Resampler.bfill#\\\\n\\\\nfinalResampler.bfill(limit=None)[source]#\\\\n\\\\nBackward fill the new missing values in the resampled data.\\\\n\\\\nIn statistics, imputation is the process of replacing missing data with\\\\nsubstituted values[1]. When resampling data, missing values may\\\\nappear (e.g., when the resampling frequency is higher than the original\\\\nfrequency). The backward fill will replace NaN values that appeared in\\\\nthe resampled data with the next value in the original sequence.\\\\nMissing values that existed in the original data will not be modified.\\\\n\\\\nParameters:\\\\n\\\\nlimitint, optional\\\\n\\\\nLimit of how many values to fill.\\\\n\\\\nReturns:\\\\n\\\\nSeries, DataFrame\\\\n\\\\nAn upsampled Series or DataFrame with backward filled NaN values.\\\\n\\\\nReferences\\\\n\\\\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\\\\n\\\\nExamples\\\\n\\\\nResampling a Series:\\\\n\\\\n>>> s = pd.Series([1, 2, 3],\\\\n...               index=pd.date_range(\\'20180101\\', periods=3, freq=\\'h\\'))\\\\n>>> s\\\\n2018-01-01 00:00:00    1\\\\n2018-01-01 01:00:00    2\\\\n2018-01-01 02:00:00    3\\\\nFreq: h, dtype: int64\\\\n\\\\n\\\\n>>> s.resample(\\'30min\\').bfill()\\\\n2018-01-01 00:00:00    1\\\\n2018-01-01 00:30:00    2\\\\n2018-01-01 01:00:00    2\\\\n2018-01-01 01:30:00    3\\\\n2018-01-01 02:00:00    3\\\\nFreq: 30min, dtype: int64\\\\n\\\\n\\\\n>>> s.resample(\\'15min\\').bfill(limit=2)\\\\n2018-01-01 00:00:00    1.0\\\\n2018-01-01 00:15:00    NaN\\\\n2018-01-01 00:30:00    2.0\\\\n2018-01-01 00:45:00    2.0\\\\n2018-01-01 01:00:00    2.0\\\\n2018-01-01 01:15:00    NaN\\\\n2018-01-01 01:30:00    3.0\\\\n2018-01-01 01:45:00    3.0\\\\n2018-01-01 02:00:00    3.0\\\\nFreq: 15min, dtype: float64\\\\n\\\\n\\\\nResampling a DataFrame that has missing values:\\\\n\\\\n>>> df = pd.DataFrame({\\'a\\': [2, np.nan, 6], \\'b\\': [1, 3, 5]},\\\\n...                   index=pd.date_range(\\'20180101\\', periods=3,\\\\n...                                       freq=\\'h\\'))\\\\n>>> df\\\\n                       a  b\\\\n2018-01-01 00:00:00  2.0  1\\\\n2018-01-01 01:00:00  NaN  3\\\\n2018-01-01 02:00:00  6.0  5\", \"type\": \"Document\"}}']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_retriever.invoke(\"how can I handle irregularly spaced time series data, including filling missing values?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44c868-e8d7-44ac-8cfb-26ee019ae128",
   "metadata": {},
   "source": [
    "### adding scikit-learn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b999b6c2-441a-46c7-8dcc-342238b5c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path\n",
    "path = '../documents/processed_docs/scikit_learn_docs.pkl'\n",
    "\n",
    "# Load data from a file\n",
    "with open(path, 'rb') as file:\n",
    "    scikit_learn_docs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "805681ee-0918-4fcb-9e9c-bbd16cc06e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scikit_learn_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0244a742-1401-4fb1-9a75-96fabc55081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding docs to local storage\n",
    "for doc in tqdm(scikit_learn_docs, desc=\"Adding documents\", unit=\"doc\"):\n",
    "    retriever.add_documents([doc], ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc9220c6-6751-4a27-a990-1027ae5705de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 19/19 [00:32<00:00,  1.73s/doc]\n"
     ]
    }
   ],
   "source": [
    "# # Adding docs to local storage\n",
    "# for doc in tqdm(scikit_learn_docs[918:], desc=\"Adding documents\", unit=\"doc\"):\n",
    "#     retriever.add_documents([doc], ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "523785bc-6b72-4386-9fe3-22f277c48693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Chroma vectorstore\n",
    "# loaded_vectorstore = Chroma(\n",
    "#     collection_name=\"split_parents\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory= child_doc_path\n",
    "# )\n",
    " \n",
    "# # Load parent document store\n",
    "# loaded_file_store = LocalFileStore(parent_doc_path)\n",
    " \n",
    "# # Recreate ParentDocumentRetriever\n",
    "# loaded_retriever = ParentDocumentRetriever(\n",
    "#     vectorstore=loaded_vectorstore,\n",
    "#     docstore=loaded_file_store,\n",
    "#     child_splitter=child_splitter,\n",
    "#     parent_splitter=parent_splitter\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c475fedc-2603-4448-80f3-4239065649d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(loaded_file_store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ff0f875-44fa-4484-a62d-52f67a8e79c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '22bc2787-24eb-4bf1-b4e3-761414464810', 'source': 'https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html', 'title': 'Comparison of kernel ridge and Gaussian process regression — scikit-learn 1.5.1 documentation'}, page_content='\"kernel using default hyperparameters\"\\n)'),\n",
       " Document(metadata={'doc_id': '739d2906-aee5-4d02-bdc3-db84142a5433', 'source': 'https://scikit-learn.org/stable/modules/grid_search.html', 'title': '3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.5.1 documentation'}, page_content='3.2.Tuning the hyper-parameters of an estimator#\\n\\nHyper-parameters are parameters that are not directly learnt within estimators.\\nIn scikit-learn they are passed as arguments to the constructor of the\\nestimator classes. Typical examples includeC,kernelandgammafor Support Vector Classifier,alphafor Lasso, etc.\\nCkernelgammaalpha\\nIt is possible and recommended to search the hyper-parameter space for the\\nbestcross validationscore.'),\n",
       " Document(metadata={'doc_id': '819d6061-55d7-4571-93f3-83d4930843f4', 'source': 'https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html', 'title': 'Lasso model selection: AIC-BIC / cross-validation — scikit-learn 1.5.1 documentation'}, page_content='Conclusion#\\n\\nIn this tutorial, we presented two approaches for selecting the best\\nhyperparameteralpha: one strategy finds the optimal value ofalphaby only using the training set and some information criterion, and another\\nstrategy is based on cross-validation.'),\n",
       " Document(metadata={'doc_id': '392e167b-c55c-4845-b9a5-5a042454600f', 'source': 'https://scikit-learn.org/stable/modules/grid_search.html', 'title': '3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.5.1 documentation'}, page_content='Comparing randomized search and grid search for hyperparameter estimationcompares the usage and efficiency\\nof randomized search and grid search.\\n\\nReferences\\n\\nBergstra, J. and Bengio, Y.,\\nRandom search for hyper-parameter optimization,\\nThe Journal of Machine Learning Research (2012)\\n\\n3.2.3.Searching for optimal parameters with successive halving#')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vectorstore.similarity_search(\"hyperparameters for linear regression model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0a55980-9fa2-4276-a860-3039ed2aac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html\", \"title\": \"pandas.json_normalize \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \">>> data = [\\\\n...     {\\\\n...         \\\\\"id\\\\\": 1,\\\\n...         \\\\\"name\\\\\": \\\\\"Cole Volk\\\\\",\\\\n...         \\\\\"fitness\\\\\": {\\\\\"height\\\\\": 130, \\\\\"weight\\\\\": 60},\\\\n...     },\\\\n...     {\\\\\"name\\\\\": \\\\\"Mark Reg\\\\\", \\\\\"fitness\\\\\": {\\\\\"height\\\\\": 130, \\\\\"weight\\\\\": 60}},\\\\n...     {\\\\n...         \\\\\"id\\\\\": 2,\\\\n...         \\\\\"name\\\\\": \\\\\"Faye Raker\\\\\",\\\\n...         \\\\\"fitness\\\\\": {\\\\\"height\\\\\": 130, \\\\\"weight\\\\\": 60},\\\\n...     },\\\\n... ]\\\\n>>> pd.json_normalize(data, max_level=0)\\\\n    id        name                        fitness\\\\n0  1.0   Cole Volk  {\\'height\\': 130, \\'weight\\': 60}\\\\n1  NaN    Mark Reg  {\\'height\\': 130, \\'weight\\': 60}\\\\n2  2.0  Faye Raker  {\\'height\\': 130, \\'weight\\': 60}\\\\n\\\\n\\\\nNormalizes nested data up to level 1.\\\\n\\\\n>>> data = [\\\\n...     {\\\\n...         \\\\\"id\\\\\": 1,\\\\n...         \\\\\"name\\\\\": \\\\\"Cole Volk\\\\\",\\\\n...         \\\\\"fitness\\\\\": {\\\\\"height\\\\\": 130, \\\\\"weight\\\\\": 60},\\\\n...     },\\\\n...     {\\\\\"name\\\\\": \\\\\"Mark Reg\\\\\", \\\\\"fitness\\\\\": {\\\\\"height\\\\\": 130, \\\\\"weight\\\\\": 60}},\\\\n...     {\\\\n...         \\\\\"id\\\\\": 2,\\\\n...         \\\\\"name\\\\\": \\\\\"Faye Raker\\\\\",\\\\n...         \\\\\"fitness\\\\\": {\\\\\"height\\\\\": 130, \\\\\"weight\\\\\": 60},\\\\n...     },\\\\n... ]\\\\n>>> pd.json_normalize(data, max_level=1)\\\\n    id        name  fitness.height  fitness.weight\\\\n0  1.0   Cole Volk             130              60\\\\n1  NaN    Mark Reg             130              60\\\\n2  2.0  Faye Raker             130              60\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.Easter.normalize.html\", \"title\": \"pandas.tseries.offsets.Easter.normalize \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"pandas.tseries.offsets.Easter.normalize#\\\\n\\\\nEaster.normalize#\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.BYearBegin.normalize.html\", \"title\": \"pandas.tseries.offsets.BYearBegin.normalize \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"pandas.tseries.offsets.BYearBegin.normalize#\\\\n\\\\nBYearBegin.normalize#\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.BQuarterBegin.normalize.html\", \"title\": \"pandas.tseries.offsets.BQuarterBegin.normalize \\\\u2014 pandas 2.2.2 documentation\"}, \"page_content\": \"pandas.tseries.offsets.BQuarterBegin.normalize#\\\\n\\\\nBQuarterBegin.normalize#\", \"type\": \"Document\"}}']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_retriever.invoke(\"I am getting an error while try to normalize the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19ed8d-e01b-4a99-a15d-a379472589a8",
   "metadata": {},
   "source": [
    "### adding numpy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0207a5ba-d8ad-411f-bd5a-5dbc8ac8943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path\n",
    "path = '../documents/processed_docs/numpy_docs.pkl'\n",
    "\n",
    "# Load data from a file\n",
    "with open(path, 'rb') as file:\n",
    "    numpy_docs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36840a6f-1bf1-427b-a8fa-31de656d17bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2464"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numpy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fb310f4-9d3d-4eb9-9ab4-c285240d6f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 2464/2464 [07:04<00:00,  5.80doc/s] \n"
     ]
    }
   ],
   "source": [
    "# Adding docs to local storage\n",
    "for doc in tqdm(numpy_docs, desc=\"Adding documents\", unit=\"doc\"):\n",
    "    retriever.add_documents([doc], ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "073b2c8f-12c4-4ab1-8588-4879773df5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13720"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(loaded_file_store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "200c3241-ccfe-493a-8bdc-834703187c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '631be762-2f59-49e2-a8f1-03975d3aa39b', 'source': 'https://numpy.org/doc/stable/reference/arrays.ndarray.html', 'title': 'The N-dimensional array (ndarray) — NumPy v2.0 Manual'}, page_content='>>> y = x[:,1]\\n>>> y\\narray([2, 5], dtype=int32)\\n>>> y[0] = 9 # this also changes the corresponding element in x\\n>>> y\\narray([9, 5], dtype=int32)\\n>>> x\\narray([[1, 9, 3],\\n       [4, 5, 6]], dtype=int32)\\n\\n\\nConstructing arrays#\\n\\nNew arrays can be constructed using the routines detailed inArray creation routines, and also by using the low-levelndarrayconstructor:\\n\\nndarray(shape[,\\xa0dtype,\\xa0buffer,\\xa0offset,\\xa0...])\\n\\nAn array object represents a multidimensional, homogeneous array of fixed-size items.'),\n",
       " Document(metadata={'doc_id': '9c040906-0485-4022-83fd-0be4b34b81ff', 'source': 'https://numpy.org/doc/stable/user/absolute_beginners.html', 'title': 'NumPy: the absolute basics for beginners — NumPy v2.0 Manual'}, page_content='How to create a basic array#\\n\\nThis section coversnp.zeros(),np.ones(),np.empty(),np.arange(),np.linspace()\\nnp.zeros()np.ones()np.empty()np.arange()np.linspace()\\nBesides creating an array from a sequence of elements, you can easily create an\\narray filled with0’s:\\n\\n>>> np.zeros(2)\\narray([0., 0.])\\n\\n\\nOr an array filled with1’s:\\n\\n>>> np.ones(2)\\narray([1., 1.])'),\n",
       " Document(metadata={'doc_id': 'f25edbdd-0140-4087-9bdf-25e83ef07349', 'source': 'https://numpy.org/doc/stable/reference/generated/numpy.ma.count.html', 'title': 'numpy.ma.count — NumPy v2.0 Manual'}, page_content='When theaxiskeyword is specified an array of appropriate size is\\nreturned.\\n\\n>>> a.count(axis=0)\\narray([1, 1, 1])\\n>>> a.count(axis=1)\\narray([3, 0])'),\n",
       " Document(metadata={'doc_id': '4f733fa7-17aa-4d14-b888-402a0cc3c4b0', 'source': 'https://numpy.org/doc/stable/reference/generated/numpy.ma.masked_array.count.html', 'title': 'numpy.ma.masked_array.count — NumPy v2.0 Manual'}, page_content='When theaxiskeyword is specified an array of appropriate size is\\nreturned.\\n\\n>>> a.count(axis=0)\\narray([1, 1, 1])\\n>>> a.count(axis=1)\\narray([3, 0])')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vectorstore.similarity_search(\"how can i create a 2d array?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb72a447-8e65-401c-b326-3d906ad6d3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://numpy.org/doc/stable/reference/arrays.ndarray.html\", \"title\": \"The N-dimensional array (ndarray) \\\\u2014 NumPy v2.0 Manual\"}, \"page_content\": \"The N-dimensional array (ndarray)#\\\\nndarray\\\\nAnndarrayis a (usually fixed-size) multidimensional\\\\ncontainer of items of the same type and size. The number of dimensions\\\\nand items in an array is defined by itsshape,\\\\nwhich is atupleofNnon-negative integers that specify the\\\\nsizes of each dimension. The type of items in the array is specified by\\\\na separatedata-type object (dtype), one of which\\\\nis associated with each ndarray.\\\\nshapetuple\\\\nAs with other container objects in Python, the contents of anndarraycan be accessed and modified byindexing or\\\\nslicingthe array (using, for example,Nintegers),\\\\nand via the methods and attributes of thendarray.\\\\n\\\\nDifferentndarrayscan share the same data, so that\\\\nchanges made in onendarraymay be visible in another. That\\\\nis, an ndarray can be a\\\\u201cview\\\\u201dto another ndarray, and the data it\\\\nis referring to is taken care of by the\\\\u201cbase\\\\u201dndarray. ndarrays can\\\\nalso be views to memory owned by Pythonstringsor\\\\nobjects implementing thememoryvieworarrayinterfaces.\\\\nndarraysstringsmemoryview\\\\nExample\\\\n\\\\nA 2-dimensional array of size 2 x 3, composed of 4-byte integer\\\\nelements:\\\\n\\\\n>>> x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\\\\n>>> type(x)\\\\n<class \\'numpy.ndarray\\'>\\\\n>>> x.shape\\\\n(2, 3)\\\\n>>> x.dtype\\\\ndtype(\\'int32\\')\\\\n\\\\n\\\\nThe array can be indexed using Python container-like syntax:\\\\n\\\\n>>> # The element of x in the *second* row, *third* column, namely, 6.\\\\n>>> x[1, 2]\\\\n6\\\\n\\\\n\\\\nFor exampleslicingcan produce views of\\\\nthe array:\\\\n\\\\n>>> y = x[:,1]\\\\n>>> y\\\\narray([2, 5], dtype=int32)\\\\n>>> y[0] = 9 # this also changes the corresponding element in x\\\\n>>> y\\\\narray([9, 5], dtype=int32)\\\\n>>> x\\\\narray([[1, 9, 3],\\\\n       [4, 5, 6]], dtype=int32)\\\\n\\\\n\\\\nConstructing arrays#\\\\n\\\\nNew arrays can be constructed using the routines detailed inArray creation routines, and also by using the low-levelndarrayconstructor:\\\\n\\\\nndarray(shape[,\\\\u00a0dtype,\\\\u00a0buffer,\\\\u00a0offset,\\\\u00a0...])\\\\n\\\\nAn array object represents a multidimensional, homogeneous array of fixed-size items.\\\\n\\\\nIndexing arrays#\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://numpy.org/doc/stable/user/absolute_beginners.html\", \"title\": \"NumPy: the absolute basics for beginners \\\\u2014 NumPy v2.0 Manual\"}, \"page_content\": \"You might hear of a 0-D (zero-dimensional) array referred to as a \\\\u201cscalar\\\\u201d,\\\\na 1-D (one-dimensional) array as a \\\\u201cvector\\\\u201d, a 2-D (two-dimensional) array\\\\nas a \\\\u201cmatrix\\\\u201d, or an N-D (N-dimensional, where \\\\u201cN\\\\u201d is typically an integer\\\\ngreater than 2) array as a \\\\u201ctensor\\\\u201d. For clarity, it is best to avoid the\\\\nmathematical terms when referring to an array because the mathematical\\\\nobjects with these names behave differently than arrays (e.g. \\\\u201cmatrix\\\\u201d\\\\nmultiplication is fundamentally different from \\\\u201carray\\\\u201d multiplication), and\\\\nthere are other objects in the scientific Python ecosystem that have these\\\\nnames (e.g. the fundamental data structure of PyTorch is the \\\\u201ctensor\\\\u201d).\\\\n\\\\nArray attributes#\\\\n\\\\nThis section covers thendim,shape,size,anddtypeattributes of an array.\\\\nndimshapesizedtype\\\\nThe number of dimensions of an array is contained in thendimattribute.\\\\n\\\\n>>> a.ndim\\\\n2\\\\n\\\\n\\\\nThe shape of an array is a tuple of non-negative integers that specify the\\\\nnumber of elements along each dimension.\\\\n\\\\n>>> a.shape\\\\n(3, 4)\\\\n>>> len(a.shape) == a.ndim\\\\nTrue\\\\n\\\\n\\\\nThe fixed, total number of elements in array is contained in thesizeattribute.\\\\n\\\\n>>> a.size\\\\n12\\\\n>>> import math\\\\n>>> a.size == math.prod(a.shape)\\\\nTrue\\\\n\\\\n\\\\nArrays are typically \\\\u201chomogeneous\\\\u201d, meaning that they contain elements of\\\\nonly one \\\\u201cdata type\\\\u201d. The data type is recorded in thedtypeattribute.\\\\n\\\\n>>> a.dtype\\\\ndtype(\\'int64\\')  # \\\\\"int\\\\\" for integer, \\\\\"64\\\\\" for 64-bit\\\\n\\\\n\\\\nRead more about array attributes hereand learn aboutarray objects here.\\\\n\\\\nHow to create a basic array#\\\\n\\\\nThis section coversnp.zeros(),np.ones(),np.empty(),np.arange(),np.linspace()\\\\nnp.zeros()np.ones()np.empty()np.arange()np.linspace()\\\\nBesides creating an array from a sequence of elements, you can easily create an\\\\narray filled with0\\\\u2019s:\\\\n\\\\n>>> np.zeros(2)\\\\narray([0., 0.])\\\\n\\\\n\\\\nOr an array filled with1\\\\u2019s:\\\\n\\\\n>>> np.ones(2)\\\\narray([1., 1.])\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://numpy.org/doc/stable/reference/generated/numpy.ma.count.html\", \"title\": \"numpy.ma.count \\\\u2014 NumPy v2.0 Manual\"}, \"page_content\": \"numpy.ma.count#\\\\n\\\\nma.count(self,axis=None,keepdims=<novalue>)=<numpy.ma.core._frommethodobject>#\\\\n\\\\nCount the non-masked elements of the array along the given axis.\\\\n\\\\nParameters:\\\\n\\\\naxisNone or int or tuple of ints, optional\\\\n\\\\nAxis or axes along which the count is performed.\\\\nThe default, None, performs the count over all\\\\nthe dimensions of the input array.axismay be negative, in\\\\nwhich case it counts from the last to the first axis.\\\\n\\\\nNew in version 1.10.0.\\\\n\\\\nIf this is a tuple of ints, the count is performed on multiple\\\\naxes, instead of a single axis or all the axes as before.\\\\n\\\\nkeepdimsbool, optional\\\\n\\\\nIf this is set to True, the axes which are reduced are left\\\\nin the result as dimensions with size one. With this option,\\\\nthe result will broadcast correctly against the array.\\\\n\\\\nReturns:\\\\n\\\\nresultndarray or scalar\\\\n\\\\nAn array with the same shape as the input array, with the specified\\\\naxis removed. If the array is a 0-d array, or ifaxisis None, a\\\\nscalar is returned.\\\\n\\\\nExamples\\\\n\\\\n>>> import numpy.ma as ma\\\\n>>> a = ma.arange(6).reshape((2, 3))\\\\n>>> a[1, :] = ma.masked\\\\n>>> a\\\\nmasked_array(\\\\n  data=[[0, 1, 2],\\\\n        [--, --, --]],\\\\n  mask=[[False, False, False],\\\\n        [ True,  True,  True]],\\\\n  fill_value=999999)\\\\n>>> a.count()\\\\n3\\\\n\\\\n\\\\nWhen theaxiskeyword is specified an array of appropriate size is\\\\nreturned.\\\\n\\\\n>>> a.count(axis=0)\\\\narray([1, 1, 1])\\\\n>>> a.count(axis=1)\\\\narray([3, 0])\", \"type\": \"Document\"}}',\n",
       " b'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://numpy.org/doc/stable/reference/generated/numpy.ma.masked_array.count.html\", \"title\": \"numpy.ma.masked_array.count \\\\u2014 NumPy v2.0 Manual\"}, \"page_content\": \"numpy.ma.masked_array.count#\\\\n\\\\nmethod\\\\n\\\\nma.masked_array.count(axis=None,keepdims=<novalue>)[source]#\\\\n\\\\nCount the non-masked elements of the array along the given axis.\\\\n\\\\nParameters:\\\\n\\\\naxisNone or int or tuple of ints, optional\\\\n\\\\nAxis or axes along which the count is performed.\\\\nThe default, None, performs the count over all\\\\nthe dimensions of the input array.axismay be negative, in\\\\nwhich case it counts from the last to the first axis.\\\\n\\\\nNew in version 1.10.0.\\\\n\\\\nIf this is a tuple of ints, the count is performed on multiple\\\\naxes, instead of a single axis or all the axes as before.\\\\n\\\\nkeepdimsbool, optional\\\\n\\\\nIf this is set to True, the axes which are reduced are left\\\\nin the result as dimensions with size one. With this option,\\\\nthe result will broadcast correctly against the array.\\\\n\\\\nReturns:\\\\n\\\\nresultndarray or scalar\\\\n\\\\nAn array with the same shape as the input array, with the specified\\\\naxis removed. If the array is a 0-d array, or ifaxisis None, a\\\\nscalar is returned.\\\\n\\\\nExamples\\\\n\\\\n>>> import numpy.ma as ma\\\\n>>> a = ma.arange(6).reshape((2, 3))\\\\n>>> a[1, :] = ma.masked\\\\n>>> a\\\\nmasked_array(\\\\n  data=[[0, 1, 2],\\\\n        [--, --, --]],\\\\n  mask=[[False, False, False],\\\\n        [ True,  True,  True]],\\\\n  fill_value=999999)\\\\n>>> a.count()\\\\n3\\\\n\\\\n\\\\nWhen theaxiskeyword is specified an array of appropriate size is\\\\nreturned.\\\\n\\\\n>>> a.count(axis=0)\\\\narray([1, 1, 1])\\\\n>>> a.count(axis=1)\\\\narray([3, 0])\", \"type\": \"Document\"}}']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_retriever.invoke(\"how can i create a 2d array?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fcb6811-8e0c-4305-aca6-87f0c72439e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParentDocumentRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x2b245f985970>, docstore=<langchain.storage.file_system.LocalFileStore object at 0x2b2452958610>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x2b2453657e20>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x2b2453657760>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455c28e-0924-4722-89dd-f9d95d16c757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
